This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*_test_*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yaml
src/
  authorization/
    routes/
      v1/
        routes.py
        schemas.py
        utils.py
    .authorization.env.example
    config.py
  core/
    .argon.env.example
    .logger.env.example
    config.py
    db_session.py
    exception_handlers.py
    logging.py
    model_base.py
    security.py
  simulations/
    core/
      distributions.py
      engine.py
      enums.py
      schemas.py
  users/
    alembic/
      versions/
        06df4ff49d9f_create_user_table.py
      env.py
      script.py.mako
    db_utils/
      exceptions.py
      users.py
    models/
      users.py
    routes/
      v1/
        routes.py
        schemas.py
    alembic.ini
  .service.env.example
  config.py
  main.py
tests/
  authorization/
    routes/
      v1/
        test_routes.py
  core/
    test_exception_handlers.py
    test_logging.py
    test_security.py
  simulations/
    core/
      test_distributions.py
      test_engine.py
  users/
    db_utils/
      test_users.py
    routes/
      v1/
        test_routes.py
  conftest.py
.env.example
.gitignore
docker-compose.dev.yaml
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/simulations/core/distributions.py">
import numpy as np
from typing import Callable
from scipy.stats import truncnorm
from src.simulations.core.enums import DistributionType
from src.simulations.core.schemas import DistributionParams
def get_generator(params: DistributionParams) -> Callable[[], float]:
    if params.distribution == DistributionType.EXPONENTIAL:
        return lambda: np.random.exponential(1.0 / params.rate)
    if params.distribution == DistributionType.UNIFORM:
        return lambda: np.random.uniform(params.a, params.b)
    if params.distribution == DistributionType.GAMMA:
        return lambda: np.random.gamma(params.k, params.theta)
    if params.distribution == DistributionType.WEIBULL:
        return lambda: params.lambda_param * np.random.weibull(params.k)
    if params.distribution == DistributionType.TRUNCATED_NORMAL:
        a_std = (params.a - params.mu) / params.sigma
        b_std = (params.b - params.mu) / params.sigma
        return lambda: truncnorm.rvs(
            a_std, b_std, loc=params.mu, scale=params.sigma
        )
    raise NotImplementedError(
        f"Distribution '{params.distribution}' is not supported."
    )
</file>

<file path="src/simulations/core/engine.py">
import heapq
import math
from typing import Callable
from scipy import stats as scipy_stats
import numpy as np
from src.simulations.core.distributions import get_generator
from src.simulations.core.enums import EventType
from src.simulations.core.schemas import (
    SimulationRequest,
    SimulationMetrics,
    GanttChartItem,
    SimulationResult,
    SimulationResponse,
    ConfidenceInterval,
    AggregatedMetrics,
)
class Simulator:
    def __init__(self, params: SimulationRequest):
        self.params = params
        if params.random_seed is not None:
            np.random.seed(params.random_seed)
        self.clock = 0.0
        self.event_queue: list = []
        self.channels_free_time = [0.0] * params.num_channels
        self.processed_schedule = None
        if self.params.arrival_schedule:
            self.processed_schedule = []
            current_time = 0.0
            for item in self.params.arrival_schedule:
                current_time += item.duration
                self.processed_schedule.append(
                    {"end_time": current_time, "rate": item.rate}
                )
        self.arrival_generator = self._get_arrival_generator()
        self.service_generator = get_generator(params.service_process)
        self.stats = {
            "total_requests": 0,
            "processed_requests": 0,
            "rejected_requests": 0,
            "total_busy_time": 0.0,
        }
        self.gantt_chart_data: list[GanttChartItem] = []
    def _get_arrival_generator(self) -> Callable[[], float]:
        if self.processed_schedule:
            def non_stationary_generator() -> float:
                current_rate = None
                for interval in self.processed_schedule:
                    if self.clock < interval["end_time"]:
                        current_rate = interval["rate"]
                        break
                if current_rate is None:
                    current_rate = self.processed_schedule[-1]["rate"]
                return np.random.exponential(1.0 / current_rate)
            return non_stationary_generator
        return get_generator(self.params.arrival_process)
    def run(self) -> SimulationResult:
        self._schedule_event(self.arrival_generator(), EventType.ARRIVAL)
        while self.event_queue and self.clock < self.params.simulation_time:
            time, event_type, data = heapq.heappop(self.event_queue)
            self.clock = time
            if self.clock > self.params.simulation_time:
                break
            if event_type == EventType.ARRIVAL:
                self._handle_arrival()
            elif event_type == EventType.DEPARTURE:
                pass
        return self._calculate_results()
    def _schedule_event(self, delay: float, event_type: str, data=None):
        heapq.heappush(
            self.event_queue, (self.clock + delay, event_type, data or {})
        )
    def _handle_arrival(self):
        self.stats["total_requests"] += 1
        self._schedule_event(self.arrival_generator(), EventType.ARRIVAL)
        earliest_free_time = min(self.channels_free_time)
        if earliest_free_time <= self.clock:
            channel_id = self.channels_free_time.index(earliest_free_time)
            self.stats["processed_requests"] += 1
            service_time = self.service_generator()
            departure_time = self.clock + service_time
            self.stats["total_busy_time"] += service_time
            self.channels_free_time[channel_id] = departure_time
            self._schedule_event(
                service_time, EventType.DEPARTURE, {"channel_id": channel_id}
            )
            self.gantt_chart_data.append(
                GanttChartItem(
                    channel=channel_id,
                    start=self.clock,
                    end=departure_time,
                    duration=service_time,
                )
            )
        else:
            self.stats["rejected_requests"] += 1
    def _calculate_results(self) -> SimulationResult:
        total_time = self.params.simulation_time
        total_channel_time = self.params.num_channels * total_time
        metrics = SimulationMetrics(
            total_requests=self.stats["total_requests"],
            processed_requests=self.stats["processed_requests"],
            rejected_requests=self.stats["rejected_requests"],
            rejection_probability=(
                self.stats["rejected_requests"] / self.stats["total_requests"]
                if self.stats["total_requests"] > 0
                else 0
            ),
            avg_channel_utilization=(
                self.stats["total_busy_time"] / total_channel_time
                if total_channel_time > 0
                else 0
            ),
            throughput=self.stats["processed_requests"] / total_time,
        )
        return SimulationResult(
            metrics=metrics, gantt_chart=self.gantt_chart_data
        )
def run_replications(params: SimulationRequest) -> SimulationResponse:
    replication_results = []
    for i in range(params.num_replications):
        if params.random_seed is not None:
            sim_params = params.model_copy(
                update={"random_seed": params.random_seed + i}, deep=True
            )
        else:
            sim_params = params
        simulator = Simulator(sim_params)
        replication_results.append(simulator.run())
    num_reps = len(replication_results)
    if num_reps < 2:
        metrics = replication_results[0].metrics
        agg_metrics = AggregatedMetrics(
            num_replications=num_reps,
            total_requests=metrics.total_requests,
            processed_requests=metrics.processed_requests,
            rejected_requests=metrics.rejected_requests,
            rejection_probability=metrics.rejection_probability,
            avg_channel_utilization=metrics.avg_channel_utilization,
            throughput=metrics.throughput,
        )
        return SimulationResponse(
            aggregated_metrics=agg_metrics,
            replications=replication_results,
        )
    rejection_probs = [
        r.metrics.rejection_probability for r in replication_results
    ]
    utilizations = [
        r.metrics.avg_channel_utilization for r in replication_results
    ]
    throughputs = [r.metrics.throughput for r in replication_results]
    mean_rejection_prob = np.mean(rejection_probs)
    mean_utilization = np.mean(utilizations)
    mean_throughput = np.mean(throughputs)
    std_rejection_prob = float(np.std(rejection_probs, ddof=1))
    std_utilization = float(np.std(utilizations, ddof=1))
    std_throughput = float(np.std(throughputs, ddof=1))
    confidence_level = 0.95
    degrees_freedom = num_reps - 1
    t_critical = scipy_stats.t.ppf((1 + confidence_level) / 2, degrees_freedom)
    sqrt_n = math.sqrt(float(num_reps))
    h_rejection = t_critical * (std_rejection_prob / sqrt_n)
    h_utilization = t_critical * (std_utilization / sqrt_n)
    h_throughput = t_critical * (std_throughput / sqrt_n)
    aggregated_metrics = AggregatedMetrics(
        num_replications=num_reps,
        total_requests=float(
            np.mean([r.metrics.total_requests for r in replication_results])
        ),
        processed_requests=float(
            np.mean([r.metrics.processed_requests for r in replication_results])
        ),
        rejected_requests=float(
            np.mean([r.metrics.rejected_requests for r in replication_results])
        ),
        rejection_probability=float(mean_rejection_prob),
        avg_channel_utilization=float(mean_utilization),
        throughput=float(mean_throughput),
        rejection_probability_ci=ConfidenceInterval(
            lower_bound=mean_rejection_prob - h_rejection,
            upper_bound=mean_rejection_prob + h_rejection,
        ),
        avg_channel_utilization_ci=ConfidenceInterval(
            lower_bound=mean_utilization - h_utilization,
            upper_bound=mean_utilization + h_utilization,
        ),
        throughput_ci=ConfidenceInterval(
            lower_bound=mean_throughput - h_throughput,
            upper_bound=mean_throughput + h_throughput,
        ),
    )
    return SimulationResponse(
        aggregated_metrics=aggregated_metrics,
        replications=replication_results,
    )
</file>

<file path="src/simulations/core/enums.py">
from enum import Enum
class DistributionType(str, Enum):
    EXPONENTIAL = "exponential"
    UNIFORM = "uniform"
    GAMMA = "gamma"
    WEIBULL = "weibull"
    TRUNCATED_NORMAL = "truncated_normal"
class EventType(str, Enum):
    ARRIVAL = "arrival"
    DEPARTURE = "departure"
</file>

<file path="src/simulations/core/schemas.py">
from typing import Union
from pydantic import Field, BaseModel, ConfigDict
from src.simulations.core.enums import DistributionType
class ExponentialParams(BaseModel):
    distribution: DistributionType = DistributionType.EXPONENTIAL
    rate: float = Field(..., gt=0, description="Rate (lambda)")
class UniformParams(BaseModel):
    distribution: DistributionType = DistributionType.UNIFORM
    a: float = Field(..., description="Lower bound")
    b: float = Field(..., ge=0, description="Upper bound")
class TruncatedNormalParams(BaseModel):
    distribution: DistributionType = DistributionType.TRUNCATED_NORMAL
    mu: float = Field(..., description="Mean (mu)")
    sigma: float = Field(..., gt=0, description="Standard deviation (sigma)")
    a: float = Field(0, description="Lower threshold")
    b: float = Field(float("inf"), description="Upper threshold")
class GammaParams(BaseModel):
    distribution: DistributionType = DistributionType.GAMMA
    k: float = Field(..., gt=0, description="Form (k)")
    theta: float = Field(..., gt=0, description="Scale (theta)")
class WeibullParams(BaseModel):
    distribution: DistributionType = DistributionType.WEIBULL
    k: float = Field(..., gt=0, description="Form (k)")
    lambda_param: float = Field(..., gt=0, description="Scale (lambda)")
DistributionParams = Union[
    ExponentialParams,
    UniformParams,
    TruncatedNormalParams,
    GammaParams,
    WeibullParams,
]
class ArrivalScheduleItem(BaseModel):
    duration: float = Field(..., gt=0, description="Interval duration")
    rate: float = Field(
        ..., gt=0, description="Rate (lambda) on a given interval"
    )
class GanttChartItem(BaseModel):
    channel: int
    start: float
    end: float
    duration: float
class SimulationMetrics(BaseModel):
    total_requests: int
    processed_requests: int
    rejected_requests: int
    rejection_probability: float
    avg_channel_utilization: float
    throughput: float
class ConfidenceInterval(BaseModel):
    lower_bound: float
    upper_bound: float
class AggregatedMetrics(BaseModel):
    num_replications: int
    total_requests: float
    processed_requests: float
    rejected_requests: float
    rejection_probability: float
    avg_channel_utilization: float
    throughput: float
    rejection_probability_ci: ConfidenceInterval | None = None
    avg_channel_utilization_ci: ConfidenceInterval | None = None
    throughput_ci: ConfidenceInterval | None = None
class SimulationRequest(BaseModel):
    num_channels: int = Field(..., gt=0, alias="numChannels")
    simulation_time: float = Field(..., gt=0, alias="simulationTime")
    num_replications: int = Field(1, ge=1, alias="numReplications")
    arrival_process: DistributionParams = Field(..., alias="arrivalProcess")
    service_process: DistributionParams = Field(..., alias="serviceProcess")
    arrival_schedule: list[ArrivalScheduleItem] | None = Field(
        None, alias="arrivalSchedule"
    )
    random_seed: int | None = Field(None, alias="randomSeed")
    model_config = ConfigDict(
        populate_by_name=True,
    )
class SimulationResult(BaseModel):
    metrics: SimulationMetrics
    gantt_chart: list[GanttChartItem]
class SimulationResponse(BaseModel):
    aggregated_metrics: AggregatedMetrics
    replications: list[SimulationResult]
</file>

<file path="src/users/routes/v1/routes.py">
from another_fastapi_jwt_auth import AuthJWT
from fastapi import Depends, APIRouter
from sqlalchemy.ext.asyncio import AsyncSession
from starlette.responses import JSONResponse
from starlette import status
from src.core.db_session import get_session
from src.users.db_utils.exceptions import UserNotFound
from src.users.db_utils.users import (
    get_user_by_email,
    delete_user,
)
from src.users.routes.v1.schemas import GetMeResponse
router = APIRouter(tags=["v1", "users"], prefix="/v1/users")
@router.get("/me", response_model=GetMeResponse, status_code=status.HTTP_200_OK)
async def get_current_user(
    authorize: AuthJWT = Depends(),
    session: AsyncSession = Depends(get_session),
):
    authorize.jwt_required()
    current_user_email = authorize.get_jwt_subject()
    try:
        user = await get_user_by_email(session, email=current_user_email)
    except UserNotFound:
        response = JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": "User not found"},
        )
        authorize.unset_jwt_cookies(response=response)
        return response
    return user
@router.delete("/me", status_code=status.HTTP_200_OK)
async def delete_current_user(
    authorize: AuthJWT = Depends(),
    session: AsyncSession = Depends(get_session),
):
    authorize.jwt_required()
    current_user_email = authorize.get_jwt_subject()
    await delete_user(session, email=current_user_email)
    response = JSONResponse(content={"detail": "User deleted"})
    authorize.unset_jwt_cookies(response=response)
    return response
</file>

<file path="src/users/routes/v1/schemas.py">
import datetime
import uuid
from pydantic import BaseModel
class GetMeResponse(BaseModel):
    id: uuid.UUID
    email: str
    is_active: bool
    created_at: datetime.datetime
    updated_at: datetime.datetime
</file>

<file path="tests/authorization/routes/v1/test_routes.py">
import pytest
from fastapi import status
from fastapi.testclient import TestClient
from unittest.mock import MagicMock
from another_fastapi_jwt_auth import AuthJWT
from src.main import app
from src.core.db_session import get_session
from src.users.db_utils.exceptions import (
    UserNotFound,
    PasswordDoesNotMatch,
    UserAlreadyExists,
    UserIsNotActive,
)
from src.users.models.users import Users
BASE_URL = "/api/v1/authorization"
TEST_EMAIL = "test@example.com"
TEST_PASSWORD = "password123"
ACCESS_TOKEN = "fake_access_token"
REFRESH_TOKEN = "fake_refresh_token"
@pytest.fixture
def client():
    return TestClient(app)
@pytest.fixture
def mock_user():
    user = Users()
    user.email = TEST_EMAIL
    return user
@pytest.fixture
def mock_auth_dependency():
    mock_authorize = MagicMock()
    mock_authorize.get_jwt_subject.return_value = TEST_EMAIL
    mock_authorize.jwt_required.return_value = None
    mock_authorize.jwt_refresh_token_required.return_value = None
    mock_authorize.unset_jwt_cookies.return_value = None
    def get_mock_authorize():
        return mock_authorize
    app.dependency_overrides[AuthJWT] = get_mock_authorize
    yield mock_authorize
    app.dependency_overrides = {}
@pytest.fixture(autouse=True)
def override_db_dependency():
    mock_session = MagicMock()
    app.dependency_overrides[get_session] = lambda: mock_session
    yield mock_session
    app.dependency_overrides = {}
def test_signin_success(client, mocker, mock_user, mock_auth_dependency):
    mocker.patch(
        "src.authorization.routes.v1.routes.get_user_by_credentials",
        return_value=mock_user,
    )
    mock_create_tokens = mocker.patch(
        "src.authorization.routes.v1.routes.create_tokens",
        return_value=(ACCESS_TOKEN, REFRESH_TOKEN),
    )
    response = client.post(
        f"{BASE_URL}/signin",
        json={"email": TEST_EMAIL, "password": TEST_PASSWORD},
    )
    assert response.status_code == status.HTTP_204_NO_CONTENT
    mock_create_tokens.assert_called_once_with(
        mock_auth_dependency, email=mock_user.email
    )
@pytest.mark.parametrize("exception", [UserNotFound, PasswordDoesNotMatch])
def test_signin_failure(client, mocker, mock_auth_dependency, exception):
    mocker.patch(
        "src.authorization.routes.v1.routes.get_user_by_credentials",
        side_effect=exception,
    )
    response = client.post(
        f"{BASE_URL}/signin",
        json={"email": TEST_EMAIL, "password": TEST_PASSWORD},
    )
    assert response.status_code == status.HTTP_404_NOT_FOUND
    assert response.json() == {"detail": "User not found"}
    mock_auth_dependency.unset_jwt_cookies.assert_called_once()
def test_signup_success(client, mocker, mock_user):
    mock_create_user = mocker.patch(
        "src.authorization.routes.v1.routes.create_user",
        return_value=mock_user,
    )
    response = client.post(
        f"{BASE_URL}/signup",
        json={"email": TEST_EMAIL, "password": TEST_PASSWORD},
    )
    assert response.status_code == status.HTTP_201_CREATED
    assert response.json() == {"email": TEST_EMAIL}
    mock_create_user.assert_called_once()
@pytest.mark.parametrize(
    "exception, expected_status, expected_detail",
    [
        (
            UserAlreadyExists,
            status.HTTP_400_BAD_REQUEST,
            "User with this email already exists.",
        ),
        (
            UserIsNotActive,
            status.HTTP_400_BAD_REQUEST,
            "User is not active. Reactivate your account.",
        ),
    ],
)
def test_signup_failure(
    client, mocker, exception, expected_status, expected_detail
):
    mocker.patch(
        "src.authorization.routes.v1.routes.create_user",
        side_effect=exception,
    )
    response = client.post(
        f"{BASE_URL}/signup",
        json={"email": TEST_EMAIL, "password": TEST_PASSWORD},
    )
    assert response.status_code == expected_status
    assert response.json() == {"detail": expected_detail}
def test_refresh_access_token_success(
    client, mocker, mock_user, mock_auth_dependency
):
    mocker.patch(
        "src.authorization.routes.v1.routes.get_user_by_email",
        return_value=mock_user,
    )
    mock_create_tokens = mocker.patch(
        "src.authorization.routes.v1.routes.create_tokens",
        return_value=(ACCESS_TOKEN, REFRESH_TOKEN),
    )
    response = client.put(f"{BASE_URL}/access-token")
    assert response.status_code == status.HTTP_204_NO_CONTENT
    mock_auth_dependency.jwt_refresh_token_required.assert_called_once()
    mock_create_tokens.assert_called_once_with(
        mock_auth_dependency, email=TEST_EMAIL
    )
def test_refresh_access_token_user_not_found(
    client, mocker, mock_auth_dependency
):
    mocker.patch(
        "src.authorization.routes.v1.routes.get_user_by_email",
        side_effect=UserNotFound,
    )
    response = client.put(f"{BASE_URL}/access-token")
    assert response.status_code == status.HTTP_404_NOT_FOUND
    assert response.json() == {"detail": "User not found"}
    mock_auth_dependency.unset_jwt_cookies.assert_called_once()
</file>

<file path="tests/core/test_exception_handlers.py">
from unittest.mock import MagicMock
from starlette.responses import JSONResponse
from src.core.exception_handlers import authjwt_exception_handler
def test_authjwt_exception_handler():
    mock_request = MagicMock()
    test_status_code = 401
    test_message = "Test Unauthorized"
    mock_exception = MagicMock()
    mock_exception.status_code = test_status_code
    mock_exception.message = test_message
    response = authjwt_exception_handler(mock_request, mock_exception)
    assert isinstance(response, JSONResponse)
    assert response.status_code == test_status_code
    assert response.body == b'{"detail":"Test Unauthorized"}'
</file>

<file path="tests/core/test_logging.py">
import logging
import structlog
from unittest.mock import MagicMock
from src.core.logging import (
    add_log_level,
    add_timestamp,
    add_logger_name,
    add_callsite,
    bind_request_context,
)
def test_add_log_level():
    event_dict = add_log_level(None, "info", {})
    assert event_dict == {"level": "INFO"}
def test_add_timestamp():
    event_dict = add_timestamp(None, None, {})
    assert "timestamp" in event_dict
    assert isinstance(event_dict["timestamp"], str)
def test_add_logger_name():
    mock_logger = MagicMock(spec=logging.Logger)
    mock_logger.name = "test_logger"
    event_dict = add_logger_name(mock_logger, None, {})
    assert event_dict == {"logger": "test_logger"}
def test_add_callsite():
    mock_record = MagicMock()
    mock_record.module = "test_module"
    mock_record.funcName = "test_func"
    mock_record.lineno = 42
    event_dict = {"_record": mock_record}
    result_dict = add_callsite(None, None, event_dict)
    assert result_dict["module"] == "test_module"
    assert result_dict["func"] == "test_func"
    assert result_dict["line"] == 42
def test_bind_request_context():
    structlog.contextvars.clear_contextvars()
    try:
        bind_request_context(request_id="123", user_id="abc")
        context = structlog.contextvars.get_contextvars()
        assert context == {"request_id": "123", "user_id": "abc"}
        bind_request_context(request_id="456")
        context = structlog.contextvars.get_contextvars()
        assert context == {"request_id": "456"}
        assert "user_id" not in context
    finally:
        structlog.contextvars.clear_contextvars()
</file>

<file path="tests/users/db_utils/test_users.py">
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from sqlalchemy.exc import IntegrityError
from src.users.db_utils.users import (
    get_user_by_email,
    get_user_by_credentials,
    create_user,
    delete_user,
)
from src.users.db_utils.exceptions import (
    UserNotFound,
    PasswordDoesNotMatch,
    UserAlreadyExists,
    UserIsNotActive,
)
from src.users.models.users import Users
TEST_EMAIL = "test@example.com"
TEST_PASSWORD = "password123"
HASHED_PASSWORD = "hashed_password"
def create_mock_user():
    user = Users()
    user.email = TEST_EMAIL
    user.password_hash = HASHED_PASSWORD
    user.is_active = True
    return user
@pytest.mark.asyncio
async def test_get_user_by_email_found():
    mock_session = AsyncMock()
    mock_user = create_mock_user()
    mock_result = MagicMock()
    mock_result.scalar_one_or_none.return_value = mock_user
    mock_session.execute.return_value = mock_result
    user = await get_user_by_email(mock_session, email=TEST_EMAIL)
    assert user == mock_user
    mock_session.execute.assert_called_once()
@pytest.mark.asyncio
async def test_get_user_by_email_not_found():
    mock_session = AsyncMock()
    mock_result = MagicMock()
    mock_result.scalar_one_or_none.return_value = None
    mock_session.execute.return_value = mock_result
    with pytest.raises(UserNotFound):
        await get_user_by_email(mock_session, email=TEST_EMAIL)
@pytest.mark.asyncio
@patch("src.users.db_utils.users.get_user_by_email")
@patch("src.users.db_utils.users.verify_password")
async def test_get_user_by_credentials_success(
    mock_verify_password, mock_get_user_by_email
):
    mock_user = create_mock_user()
    mock_get_user_by_email.return_value = mock_user
    mock_verify_password.return_value = True
    mock_session = AsyncMock()
    user = await get_user_by_credentials(
        mock_session, email=TEST_EMAIL, password=TEST_PASSWORD
    )
    assert user == mock_user
    mock_get_user_by_email.assert_called_once_with(
        mock_session, email=TEST_EMAIL
    )
    mock_verify_password.assert_called_once_with(
        TEST_PASSWORD, password_hash=HASHED_PASSWORD
    )
@pytest.mark.asyncio
@patch("src.users.db_utils.users.get_user_by_email", side_effect=UserNotFound)
async def test_get_user_by_credentials_not_found(_):
    mock_session = AsyncMock()
    with pytest.raises(UserNotFound):
        await get_user_by_credentials(
            mock_session, email=TEST_EMAIL, password=TEST_PASSWORD
        )
@pytest.mark.asyncio
@patch("src.users.db_utils.users.get_user_by_email")
@patch("src.users.db_utils.users.verify_password", return_value=False)
async def test_get_user_by_credentials_password_mismatch(
    mock_verify_password, mock_get_user_by_email
):
    mock_get_user_by_email.return_value = create_mock_user()
    mock_session = AsyncMock()
    with pytest.raises(PasswordDoesNotMatch):
        await get_user_by_credentials(
            mock_session, email=TEST_EMAIL, password="wrongpassword"
        )
@pytest.mark.asyncio
@patch(
    "src.users.db_utils.users.get_user_by_email",
    side_effect=UserNotFound,
)
@patch("src.users.db_utils.users.hash_password", return_value=HASHED_PASSWORD)
async def test_create_user_success(mock_hash_password, mock_get_user_by_email):
    mock_session = AsyncMock()
    mock_session.add = MagicMock()
    user = await create_user(
        mock_session, email=TEST_EMAIL, password=TEST_PASSWORD
    )
    mock_get_user_by_email.assert_called_once_with(
        mock_session, email=TEST_EMAIL
    )
    mock_hash_password.assert_called_once_with(TEST_PASSWORD)
    mock_session.add.assert_called_once()
    mock_session.commit.assert_called_once()
    assert user.email == TEST_EMAIL
    assert user.password_hash == HASHED_PASSWORD
    assert user.is_active is True
@pytest.mark.asyncio
@patch("src.users.db_utils.users.get_user_by_email")
async def test_create_user_already_exists(mock_get_user_by_email):
    mock_get_user_by_email.return_value = create_mock_user()
    mock_session = AsyncMock()
    with pytest.raises(UserAlreadyExists):
        await create_user(
            mock_session, email=TEST_EMAIL, password=TEST_PASSWORD
        )
@pytest.mark.asyncio
@patch(
    "src.users.db_utils.users.get_user_by_email",
    side_effect=UserNotFound,
)
@patch("src.users.db_utils.users.hash_password")
async def test_create_user_inactive_user_exists(
    mock_hash_password, mock_get_user_by_email
):
    mock_session = AsyncMock()
    mock_session.add = MagicMock()
    mock_session.commit.side_effect = IntegrityError(None, None, None)
    with pytest.raises(UserIsNotActive):
        await create_user(
            mock_session, email=TEST_EMAIL, password=TEST_PASSWORD
        )
@pytest.mark.asyncio
async def test_delete_user():
    mock_session = AsyncMock()
    await delete_user(mock_session, email=TEST_EMAIL)
    mock_session.execute.assert_called_once()
    mock_session.commit.assert_called_once()
</file>

<file path="tests/users/routes/v1/test_routes.py">
import uuid
import datetime
import pytest
from fastapi.testclient import TestClient
from unittest.mock import MagicMock
from another_fastapi_jwt_auth import AuthJWT
from src.main import app
from src.core.db_session import get_session
from src.users.db_utils.exceptions import UserNotFound
from src.users.models.users import Users
BASE_URL = "/api/v1/users"
TEST_EMAIL = "test@example.com"
@pytest.fixture
def client():
    return TestClient(app)
@pytest.fixture
def mock_user():
    user = Users()
    user.id = uuid.uuid4()
    user.email = TEST_EMAIL
    user.is_active = True
    user.created_at = datetime.datetime.now(datetime.timezone.utc)
    user.updated_at = datetime.datetime.now(datetime.timezone.utc)
    return user
@pytest.fixture
def mock_auth_dependency():
    mock_authorize = MagicMock()
    mock_authorize.get_jwt_subject.return_value = TEST_EMAIL
    mock_authorize.jwt_required.return_value = None
    mock_authorize.unset_jwt_cookies.return_value = None
    def get_mock_authorize():
        return mock_authorize
    app.dependency_overrides[AuthJWT] = get_mock_authorize
    yield mock_authorize
    app.dependency_overrides = {}
@pytest.fixture(autouse=True)
def override_db_dependency(mocker):
    mock_session = MagicMock()
    app.dependency_overrides[get_session] = lambda: mock_session
    yield mock_session
    app.dependency_overrides = {}
def test_get_current_user_success(
    client, mocker, mock_user, mock_auth_dependency
):
    mock_get_user = mocker.patch(
        "src.users.routes.v1.routes.get_user_by_email",
        return_value=mock_user,
    )
    response = client.get(f"{BASE_URL}/me")
    assert response.status_code == 200
    mock_auth_dependency.jwt_required.assert_called_once()
    mock_get_user.assert_called_once_with(mocker.ANY, email=TEST_EMAIL)
    data = response.json()
    assert data["email"] == mock_user.email
    assert data["id"] == str(mock_user.id)
    assert data["is_active"] is True
def test_get_current_user_not_found(client, mocker, mock_auth_dependency):
    mocker.patch(
        "src.users.routes.v1.routes.get_user_by_email",
        side_effect=UserNotFound,
    )
    response = client.get(f"{BASE_URL}/me")
    assert response.status_code == 404
    assert response.json() == {"detail": "User not found"}
    mock_auth_dependency.unset_jwt_cookies.assert_called_once()
def test_delete_current_user_success(client, mocker, mock_auth_dependency):
    mock_delete_user = mocker.patch("src.users.routes.v1.routes.delete_user")
    response = client.delete(f"{BASE_URL}/me")
    assert response.status_code == 200
    assert response.json() == {"detail": "User deleted"}
    mock_delete_user.assert_called_once_with(mocker.ANY, email=TEST_EMAIL)
    mock_auth_dependency.unset_jwt_cookies.assert_called_once()
</file>

<file path="tests/conftest.py">
import pytest
import src.config
from src.config import (
    ArgonSettings,
    AuthorizationSettings,
    LoggerSettings,
    ServiceSettings,
    Settings,
)
@pytest.fixture(scope="session", autouse=True)
def mock_settings():
    original_get_settings = src.config.get_settings
    def get_mock_settings():
        return Settings(
            service=ServiceSettings(
                db_url="postgresql+asyncpg://test:test@localhost:5432/testdb"
            ),
            authorization=AuthorizationSettings(
                authjwt_secret_key="test-secret-key"
            ),
            logger_settings=LoggerSettings(),
            argon_settings=ArgonSettings(),
        )
    try:
        src.config.get_settings = get_mock_settings
        yield
    finally:
        src.config.get_settings = original_get_settings
</file>

<file path=".env.example">
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=mmcc
POSTGRES_PORT=5432
</file>

<file path="src/authorization/.authorization.env.example">
AUTHJWT_SECRET_KEY="123"
</file>

<file path="src/core/.argon.env.example">
ARGON_TIME_COST=3[default=3]
ARGON_MEMORY_COST=1234[default=2**16]
ARGON_PARALLELISM=2[default=2]
</file>

<file path="src/core/.logger.env.example">
LOG_LEVEL="DEBUG"[default="INFO"]
LOG_JSON=true[default=true]
</file>

<file path="src/.service.env.example">
CORS_ORIGINS="http://localhost:3000"
DB_URL="postgresql+asyncpg://postgres:postgres@localhost:5432/mmcc"
</file>

<file path="tests/core/test_security.py">
from unittest.mock import patch
from argon2.exceptions import VerifyMismatchError
from src.core.security import hash_password, verify_password
TEST_PASSWORD = "mysecretpassword"
def test_hash_password():
    hashed = hash_password(TEST_PASSWORD)
    assert isinstance(hashed, str)
    assert hashed != TEST_PASSWORD
    assert len(hashed) > 0
def test_verify_password_success():
    hashed = hash_password(TEST_PASSWORD)
    assert verify_password(TEST_PASSWORD, password_hash=hashed) is True
def test_verify_password_failure_wrong_password():
    hashed = hash_password(TEST_PASSWORD)
    assert verify_password("wrongpassword", password_hash=hashed) is False
def test_verify_password_failure_invalid_hash():
    assert verify_password(TEST_PASSWORD, password_hash="invalidhash") is False
@patch("src.core.security.get_password_hasher")
def test_verify_password_handles_argon_exception(mock_get_password_hasher):
    mock_get_password_hasher.return_value.verify.side_effect = VerifyMismatchError
    result = verify_password("anypassword", password_hash="anyhash")
    assert result is False
</file>

<file path="tests/simulations/core/test_distributions.py">
import pytest
from typing import Callable
from src.simulations.core.distributions import get_generator
from src.simulations.core.enums import DistributionType
from src.simulations.core.schemas import (
    ExponentialParams,
    UniformParams,
    GammaParams,
    WeibullParams,
    TruncatedNormalParams,
)
@pytest.mark.parametrize(
    "params",
    [
        ExponentialParams(rate=1.0),
        UniformParams(a=1.0, b=2.0),
        GammaParams(k=1.0, theta=1.0),
        WeibullParams(k=1.0, lambda_param=1.0),
        TruncatedNormalParams(mu=5, sigma=2, a=0, b=10),
    ],
)
def test_get_generator_returns_callable(params):
    generator = get_generator(params)
    assert isinstance(generator, Callable)
@pytest.mark.parametrize(
    "params",
    [
        ExponentialParams(rate=1.0),
        UniformParams(a=1.0, b=2.0),
        GammaParams(k=1.0, theta=1.0),
        WeibullParams(k=1.0, lambda_param=1.0),
        TruncatedNormalParams(mu=5, sigma=2, a=0, b=10),
    ],
)
def test_generator_returns_positive_float(params):
    generator = get_generator(params)
    value = generator()
    assert isinstance(value, float)
    assert value >= 0.0
def test_get_generator_raises_not_implemented():
    class FakeParams:
        distribution = "fake_distribution"
    with pytest.raises(NotImplementedError):
        get_generator(FakeParams())
</file>

<file path="tests/simulations/core/test_engine.py">
from src.simulations.core.engine import Simulator, run_replications
from src.simulations.core.schemas import (
    SimulationRequest,
    ExponentialParams,
    ArrivalScheduleItem,
)
def create_base_request(**kwargs):
    defaults = {
        "num_channels": 1,
        "simulation_time": 10,
        "num_replications": 1,
        "arrival_process": ExponentialParams(rate=1.0),
        "service_process": ExponentialParams(rate=2.0),
    }
    defaults.update(kwargs)
    return SimulationRequest.model_validate(defaults)
def test_simulator_deterministic_run(mocker):
    arrival_mock = mocker.patch(
        "src.simulations.core.engine.get_generator", return_value=lambda: 1.5
    )
    service_mock = mocker.patch(
        "src.simulations.core.distributions.get_generator",
        return_value=lambda: 0.5,
    )
    params = create_base_request(simulation_time=4)
    simulator = Simulator(params)
    simulator.arrival_generator = arrival_mock.return_value
    simulator.service_generator = service_mock.return_value
    result = simulator.run()
    assert result.metrics.total_requests == 2
    assert result.metrics.processed_requests == 2
    assert result.metrics.rejected_requests == 0
def test_rejection_logic(mocker):
    arrival_mock = mocker.patch(
        "src.simulations.core.engine.get_generator", return_value=lambda: 1.0
    )
    service_mock = mocker.patch(
        "src.simulations.core.distributions.get_generator",
        return_value=lambda: 1.5,
    )
    params = create_base_request(num_channels=1, simulation_time=2.5)
    simulator = Simulator(params)
    simulator.arrival_generator = arrival_mock.return_value
    simulator.service_generator = service_mock.return_value
    result = simulator.run()
    assert result.metrics.total_requests == 2
    assert result.metrics.processed_requests == 1
    assert result.metrics.rejected_requests == 1
def test_multi_channel_logic(mocker):
    arrival_mock = mocker.patch(
        "src.simulations.core.engine.get_generator",
        return_value=lambda: 1.0,
    )
    service_mock = mocker.patch(
        "src.simulations.core.distributions.get_generator",
        return_value=lambda: 1.5,
    )
    params = create_base_request(num_channels=2, simulation_time=2.5)
    simulator = Simulator(params)
    simulator.arrival_generator = arrival_mock.return_value
    simulator.service_generator = service_mock.return_value
    result = simulator.run()
    assert result.metrics.total_requests == 2
    assert result.metrics.processed_requests == 2
    assert result.metrics.rejected_requests == 0
def test_random_seed_consistency():
    params = create_base_request(random_seed=42, num_replications=5)
    response1 = run_replications(params)
    response2 = run_replications(params)
    assert response1 == response2
def test_run_replications_aggregation():
    params = create_base_request(num_replications=10, random_seed=123)
    response = run_replications(params)
    assert len(response.replications) == 10
    agg = response.aggregated_metrics
    assert agg.num_replications == 10
    assert agg.rejection_probability_ci is not None
    assert agg.avg_channel_utilization_ci is not None
    assert agg.rejection_probability_ci.lower_bound <= agg.rejection_probability
    assert agg.rejection_probability <= agg.rejection_probability_ci.upper_bound
def test_non_stationary_flow_deterministic(mocker):
    mocker.patch("numpy.random.exponential", side_effect=lambda scale: scale)
    params = create_base_request(
        simulation_time=8,
        arrival_schedule=[
            ArrivalScheduleItem(duration=5, rate=1.0),
            ArrivalScheduleItem(duration=5, rate=4.0),
        ],
    )
    simulator = Simulator(params)
    result = simulator.run()
    assert result.metrics.total_requests == 17
</file>

<file path=".github/workflows/ci.yaml">
name: Python CI
on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.13"]
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
      - name: Run tests
        run: |
          python -m pytest
      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
</file>

<file path="src/users/alembic/versions/06df4ff49d9f_create_user_table.py">
from typing import Sequence, Union
from alembic import op
import sqlalchemy as sa
revision: str = "06df4ff49d9f"
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None
def upgrade() -> None:
    op.create_table(
        "users",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("email", sa.String(length=320), nullable=False),
        sa.Column("password_hash", sa.String(length=512), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_users")),
    )
    op.create_index(op.f("ix_users_email"), "users", ["email"], unique=True)
def downgrade() -> None:
    op.drop_index(op.f("ix_users_email"), table_name="users")
    op.drop_table("users")
</file>

<file path="src/users/alembic/env.py">
import asyncio
import os
from logging.config import fileConfig
from dotenv import load_dotenv
from sqlalchemy import Connection
from sqlalchemy import pool
from alembic import context
from sqlalchemy.ext.asyncio import async_engine_from_config
from src.config import get_settings
from src.users.models.users import *
load_dotenv(os.path.join("..", "..", ".service.env"))
config = context.config
section = config.config_ini_section
if config.config_file_name is not None:
    fileConfig(config.config_file_name)
target_metadata = Base.metadata
DB_URL = get_settings().service.db_url
config.set_main_option("sqlalchemy.url", DB_URL)
def run_migrations_offline() -> None:
    context.configure(
        url=config.get_main_option("sqlalchemy.url"),
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        include_schemas=True,
    )
    with context.begin_transaction():
        context.run_migrations()
def do_run_migrations(connection: Connection) -> None:
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,
        include_schemas=True,
    )
    with context.begin_transaction():
        context.run_migrations()
async def run_async_migrations() -> None:
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)
    await connectable.dispose()
def run_migrations_online() -> None:
    asyncio.run(run_async_migrations())
if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="src/users/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="src/users/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="docker-compose.dev.yaml">
services:
  postgres:
    image: postgres:18
    container_name: dev-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-app_db}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=C --lc-ctype=C"
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-app_db}"]
      interval: 5s
      timeout: 5s
      retries: 10
volumes:
  pg_data:
</file>

<file path="README.md">
# backend-mmcc

[![Python CI](https://github.com/rupeq/mmcc-backend/actions/workflows/ci.yaml/badge.svg)](https://github.com/rupeq/mmcc-backend/actions/workflows/ci.yaml) [![codecov](https://codecov.io/gh/rupeq/mmcc-backend/graph/badge.svg)](https://codecov.io/gh/rupeq/mmcc-backend)
</file>

<file path="src/authorization/routes/v1/schemas.py">
from pydantic import BaseModel, EmailStr
class SignInRequestSchema(BaseModel):
    email: EmailStr
    password: str
class SignUpRequestSchema(BaseModel):
    email: EmailStr
    password: str
class SignUpResponseSchema(BaseModel):
    email: EmailStr
</file>

<file path="src/authorization/routes/v1/utils.py">
from datetime import timedelta
from another_fastapi_jwt_auth import AuthJWT
from starlette.responses import Response
from src.config import get_settings
def create_tokens(authorize: AuthJWT, *, email: str) -> tuple[str, str]:
    return (
        authorize.create_access_token(
            subject=email,
            expires_time=timedelta(
                minutes=get_settings().authorization.access_token_max_age_in_minutes
            ),
            fresh=True,
        ),
        authorize.create_refresh_token(
            subject=email,
            expires_time=timedelta(
                minutes=get_settings().authorization.refresh_token_max_age_in_minutes
            ),
        ),
    )
def get_response_with_tokens(
    authorize: AuthJWT, *, response: Response, token: str, refresh_token: str
) -> Response:
    authorize.set_access_cookies(
        token,
        response=response,
        max_age=get_settings().authorization.access_token_max_age_in_minutes
        * 60,
    )
    authorize.set_refresh_cookies(
        refresh_token,
        response=response,
        max_age=get_settings().authorization.refresh_token_max_age_in_minutes
        * 60,
    )
    return response
</file>

<file path="src/authorization/config.py">
import os
from functools import lru_cache
from pydantic_settings import BaseSettings, SettingsConfigDict
class Settings(BaseSettings):
    authjwt_secret_key: str
    authjwt_token_location: set = {"cookies"}
    authjwt_cookie_secure: bool = False
    access_token_max_age_in_minutes: int = 15
    refresh_token_max_age_in_minutes: int = 60 * 24 * 7
    model_config = SettingsConfigDict(
        env_file=os.path.join(os.path.dirname(__file__), ".authorization.env")
    )
@lru_cache()
def get_config() -> Settings:
    settings = Settings()
    return settings
</file>

<file path="src/core/config.py">
import os
from functools import lru_cache
from pydantic_settings import BaseSettings, SettingsConfigDict
class LoggerSettings(BaseSettings):
    log_level: str = "INFO"
    log_json: bool = True
    model_config = SettingsConfigDict(
        env_file=os.path.join(os.path.dirname(__file__), ".logger.env")
    )
class ArgonSettings(BaseSettings):
    argon_time_cost: int = 3
    argon_memory_cost: int = 2**16
    argon_parallelism: int = 2
    model_config = SettingsConfigDict(
        env_file=os.path.join(os.path.dirname(__file__), ".argon.env")
    )
@lru_cache()
def get_logger_settings() -> LoggerSettings:
    return LoggerSettings()
@lru_cache()
def get_argon_settings() -> ArgonSettings:
    return ArgonSettings()
</file>

<file path="src/core/exception_handlers.py">
from another_fastapi_jwt_auth.exceptions import AuthJWTException
from starlette.requests import Request
from starlette.responses import JSONResponse
def authjwt_exception_handler(_: Request, exc: AuthJWTException):
    return JSONResponse(
        status_code=exc.status_code, content={"detail": exc.message}
    )
</file>

<file path="src/core/logging.py">
import logging
import logging.config
import sys
from typing import Any
import structlog
from pythonjsonlogger import json as logging_json
from src.config import get_settings
def add_service_info(_, __, event_dict: dict[str, Any]) -> dict[str, Any]:
    event_dict.setdefault("service", "smo-sim-api")
    event_dict.setdefault("env", get_settings().service.env)
    return event_dict
def add_log_level(
    _, method_name: str, event_dict: dict[str, Any]
) -> dict[str, Any]:
    event_dict["level"] = method_name.upper()
    return event_dict
def add_timestamp(_, __, event_dict: dict[str, Any]) -> dict[str, Any]:
    event_dict["timestamp"] = structlog.processors.TimeStamper(fmt="iso")(
        None, "", {}
    )["timestamp"]
    return event_dict
def add_logger_name(
    logger: logging.Logger, _, event_dict: dict[str, Any]
) -> dict[str, Any]:
    if logger:
        event_dict["logger"] = logger.name
    return event_dict
def add_callsite(_, __, event_dict: dict[str, Any]) -> dict[str, Any]:
    record = event_dict.get("_record")
    if record:
        event_dict["module"] = record.module
        event_dict["func"] = record.funcName
        event_dict["line"] = record.lineno
    return event_dict
def configure_logging() -> None:
    log_level = get_settings().logger_settings.log_level.upper()
    is_dev = (
        get_settings().service.env == "dev"
        and not get_settings().logger_settings.log_json
    )
    if is_dev:
        console_renderer = structlog.dev.ConsoleRenderer(
            colors=True, sort_keys=False
        )
    else:
        console_renderer = structlog.processors.JSONRenderer(
            serializer=lambda obj: logging_json.dumps(obj, ensure_ascii=False)
        )
    logging.basicConfig(level=log_level, stream=sys.stdout)
    for noisy in [
        "uvicorn.error",
        "uvicorn.access",
        "gunicorn.error",
        "asyncio",
    ]:
        logging.getLogger(noisy).setLevel(
            logging.INFO if is_dev else logging.WARN
        )
    shared_processors = [
        add_service_info,
        add_log_level,
        add_timestamp,
        add_logger_name,
        add_callsite,
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
    ]
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            *shared_processors,
            console_renderer,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(logging, log_level, logging.INFO)
        ),
        cache_logger_on_first_use=True,
    )
    class StructlogJSONFormatter(logging.Formatter):
        def format(self, record):
            event_dict = {
                "event": record.getMessage(),
                "_record": record,
            }
            for proc in shared_processors:
                event_dict = proc(
                    logging.getLogger(record.name),
                    record.levelname.lower(),
                    event_dict,
                )
            if isinstance(console_renderer, structlog.processors.JSONRenderer):
                return console_renderer(event_dict)
            else:
                return console_renderer(event_dict)
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(getattr(logging, log_level, logging.INFO))
    handler.setFormatter(StructlogJSONFormatter())
    root = logging.getLogger()
    if not any(isinstance(h, logging.StreamHandler) for h in root.handlers):
        root.addHandler(handler)
def bind_request_context(
    request_id: str | None = None, user_id: str | None = None
) -> None:
    structlog.contextvars.clear_contextvars()
    if request_id:
        structlog.contextvars.bind_contextvars(request_id=request_id)
    if user_id:
        structlog.contextvars.bind_contextvars(user_id=user_id)
</file>

<file path="src/core/model_base.py">
from sqlalchemy.orm import DeclarativeBase, declared_attr
from sqlalchemy import MetaData
convention = {
    "ix": "ix_%(column_0_label)s",
    "uq": "uq_%(table_name)s_%(column_0_name)s",
    "ck": "ck_%(table_name)s_%(constraint_name)s",
    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
    "pk": "pk_%(table_name)s",
}
metadata = MetaData(naming_convention=convention)
class Base(DeclarativeBase):
    metadata = metadata
    @declared_attr.directive
    def __tablename__(cls) -> str:
        return cls.__name__.lower()
</file>

<file path="src/users/models/users.py">
import uuid
from sqlalchemy import Boolean, String, func, DateTime
from sqlalchemy.orm import Mapped, mapped_column
from src.core.model_base import Base
class Users(Base):
    id: Mapped[uuid.UUID] = mapped_column(default=uuid.uuid4, primary_key=True)
    email: Mapped[str] = mapped_column(String(320), unique=True, index=True)
    password_hash: Mapped[str] = mapped_column(String(512))
    is_active: Mapped[bool] = mapped_column(Boolean, default=True)
    created_at: Mapped["DateTime"] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
    updated_at: Mapped["DateTime"] = mapped_column(
        DateTime(timezone=True), server_default=func.now(), onupdate=func.now()
    )
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
**/*.env
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer,
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/
</file>

<file path="src/core/db_session.py">
from functools import lru_cache
from typing import Any, AsyncGenerator
from sqlalchemy.ext.asyncio import (
    async_sessionmaker,
    create_async_engine,
    AsyncSession,
    AsyncEngine,
)
from src.config import get_settings
@lru_cache
def get_engine() -> AsyncEngine:
    return create_async_engine(
        get_settings().service.db_url,
        future=True,
        echo=False,
        pool_pre_ping=True,
    )
@lru_cache
def get_session_local() -> async_sessionmaker[AsyncSession]:
    return async_sessionmaker(get_engine(), expire_on_commit=False)
async def get_session() -> AsyncGenerator[AsyncSession, Any]:
    session_local = get_session_local()
    async with session_local() as session:
        yield session
</file>

<file path="src/core/security.py">
from functools import lru_cache
from argon2 import PasswordHasher
from argon2.low_level import Type
from src.config import get_settings
@lru_cache
def get_password_hasher() -> PasswordHasher:
    settings = get_settings().argon_settings
    return PasswordHasher(
        time_cost=settings.argon_time_cost,
        memory_cost=settings.argon_memory_cost,
        parallelism=settings.argon_parallelism,
        hash_len=32,
        type=Type.ID,
    )
def hash_password(password: str) -> str:
    return get_password_hasher().hash(password)
def verify_password(password: str, *, password_hash: str) -> bool:
    try:
        return get_password_hasher().verify(password_hash, password)
    except Exception:
        return False
</file>

<file path="src/users/db_utils/exceptions.py">
class UserNotFound(Exception):
    pass
class PasswordDoesNotMatch(Exception):
    pass
class UserAlreadyExists(Exception):
    pass
class UserIsNotActive(Exception):
    pass
</file>

<file path="src/main.py">
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from another_fastapi_jwt_auth.exceptions import AuthJWTException
from src.config import get_settings
from src.core.exception_handlers import authjwt_exception_handler
from src.authorization.routes.v1.routes import router as authorization_router
from src.users.routes.v1.routes import router as users_router
from src.core.logging import configure_logging
configure_logging()
app = FastAPI(
    debug=get_settings().service.debug,
    title=get_settings().service.app_name,
    docs_url="/docs" if get_settings().service.debug else None,
    redoc_url="/redoc" if get_settings().service.debug else None,
    openapi_url="/openapi" if get_settings().service.debug else None,
)
if cors_origins := get_settings().service.cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[str(cors_origin) for cors_origin in cors_origins],
        allow_credentials=get_settings().service.cors_allow_credentials,
        allow_methods=["*"],
        allow_headers=["*"],
    )
app.exception_handler(AuthJWTException)(authjwt_exception_handler)
app.include_router(
    authorization_router, prefix=get_settings().service.api_prefix
)
app.include_router(users_router, prefix=get_settings().service.api_prefix)
</file>

<file path="src/authorization/routes/v1/routes.py">
from another_fastapi_jwt_auth import AuthJWT
from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from starlette import status
from starlette.responses import Response, JSONResponse
from src.authorization.routes.v1.schemas import (
    SignInRequestSchema,
    SignUpRequestSchema,
    SignUpResponseSchema,
)
from src.authorization.routes.v1.utils import (
    create_tokens,
    get_response_with_tokens,
)
from src.core.db_session import get_session
from src.users.db_utils.exceptions import (
    UserNotFound,
    PasswordDoesNotMatch,
    UserAlreadyExists,
    UserIsNotActive,
)
from src.users.db_utils.users import (
    get_user_by_credentials,
    create_user,
    get_user_by_email,
)
router = APIRouter(tags=["v1", "authorization"], prefix="/v1/authorization")
@router.post("/signin")
async def signin(
    body: SignInRequestSchema,
    authorize: AuthJWT = Depends(),
    session: AsyncSession = Depends(get_session),
):
    try:
        user = await get_user_by_credentials(
            session, email=str(body.email), password=body.password
        )
    except (UserNotFound, PasswordDoesNotMatch):
        response = JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": "User not found"},
        )
        authorize.unset_jwt_cookies(response=response)
        return response
    token, refresh_token = create_tokens(authorize, email=user.email)
    return get_response_with_tokens(
        authorize,
        response=Response(status_code=status.HTTP_204_NO_CONTENT),
        token=token,
        refresh_token=refresh_token,
    )
@router.post(
    "/signup",
    response_model=SignUpResponseSchema,
    status_code=status.HTTP_201_CREATED,
)
async def signup(
    body: SignUpRequestSchema,
    session: AsyncSession = Depends(get_session),
):
    try:
        return await create_user(
            session, email=str(body.email), password=body.password
        )
    except UserAlreadyExists:
        response = JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"detail": "User with this email already exists."},
        )
        return response
    except UserIsNotActive:
        response = JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"detail": "User is not active. Reactivate your account."},
        )
        return response
@router.put("/access-token")
async def refresh_access_token(
    session: AsyncSession = Depends(get_session),
    authorize: AuthJWT = Depends(),
):
    authorize.jwt_refresh_token_required()
    current_user_email = authorize.get_jwt_subject()
    try:
        await get_user_by_email(session, email=current_user_email)
    except UserNotFound:
        response = JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": "User not found"},
        )
        authorize.unset_jwt_cookies(response=response)
        return response
    token, refresh_token = create_tokens(authorize, email=current_user_email)
    return get_response_with_tokens(
        authorize,
        response=Response(status_code=status.HTTP_204_NO_CONTENT),
        token=token,
        refresh_token=refresh_token,
    )
</file>

<file path="src/users/db_utils/users.py">
import logging
from sqlalchemy import select, update
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession
from src.core.security import verify_password, hash_password
from src.users.db_utils.exceptions import (
    PasswordDoesNotMatch,
    UserNotFound,
    UserAlreadyExists,
    UserIsNotActive,
)
from src.users.models.users import Users
logger = logging.getLogger(__name__)
async def get_user_by_email(session: AsyncSession, *, email: str) -> Users:
    logger.debug("Trying to get user by email: %s", email)
    maybe_user = (
        await session.execute(
            select(Users).where(Users.email == email, Users.is_active == True)
        )
    ).scalar_one_or_none()
    if maybe_user is None:
        logger.debug("User not found (email: %s)", email)
        raise UserNotFound()
    logger.debug("Found user by email: %s", email)
    return maybe_user
async def get_user_by_credentials(
    session: AsyncSession, *, email: str, password: str
) -> Users:
    logger.debug("Trying to verify user by credentials (email: %s)", email)
    user = await get_user_by_email(session, email=email)
    if not verify_password(password, password_hash=user.password_hash):
        logger.debug(
            "Found user by email (email: %s), but unable to verify them.", email
        )
        raise PasswordDoesNotMatch()
    logger.debug("Verified user using credentials (email: %s)", email)
    return user
async def create_user(
    session: AsyncSession, *, email: str, password: str
) -> Users:
    logger.debug("Trying to create user (email: %s)", email)
    try:
        await get_user_by_email(session, email=email)
        logger.debug("Found user by email (email: %s), creation failed.", email)
        raise UserAlreadyExists()
    except UserNotFound:
        pass
    user = Users(
        email=email,
        password_hash=hash_password(password),
        is_active=True,
    )
    session.add(user)
    try:
        await session.commit()
    except IntegrityError:
        logger.debug("Found inactive user (email: %s)", email)
        raise UserIsNotActive()
    logger.debug("Created user by email (email: %s).", email)
    return user
async def delete_user(session: AsyncSession, *, email: str) -> None:
    logger.debug("Trying to delete user (email: %s)", email)
    await session.execute(
        update(Users).where(Users.email == email).values(is_active=False)
    )
    await session.commit()
    logger.debug("Deleted user by email (email: %s).", email)
</file>

<file path="pyproject.toml">
[project]
name = "backend-mmcc"
version = "0.1.0"
description = "Add your description here"
requires-python = ">=3.13"
dependencies = [
    "alembic>=1.16.5",
    "another-fastapi-jwt-auth>=0.1.6",
    "argon2-cffi>=23.1.0",
    "asyncpg>=0.30.0",
    "fastapi>=0.117.1",
    "greenlet>=3.2.4",
    "httpx>=0.28.1",
    "numpy>=2.3.3",
    "pydantic-settings>=2.11.0",
    "pydantic[email]>=2.11.9",
    "pytest-asyncio>=1.2.0",
    "python-json-logger>=3.3.0",
    "scipy>=1.16.2",
    "sqlalchemy>=2.0.43",
    "structlog>=25.4.0",
    "uvicorn>=0.37.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.4.2",
    "pytest-mock>=3.15.1",
    "ruff>=0.13.2",
    "pytest-cov>=7.0.0"
]

[tool.ruff]
line-length = 80

[tool.pytest.ini_options]
addopts = "--cov=src --cov-report=xml"

[tool.coverage.run]
source = ["src"]

[tool.coverage.report]
fail_under = 90
</file>

<file path="src/config.py">
import os
from functools import lru_cache
from typing import Literal, Any
from another_fastapi_jwt_auth import AuthJWT
from pydantic import Field, AnyHttpUrl
from pydantic_settings import BaseSettings, SettingsConfigDict
from src.authorization.config import (
    Settings as AuthorizationSettings,
    get_config as get_authorization_config,
)
from src.core.config import (
    get_logger_settings,
    LoggerSettings,
    get_argon_settings,
    ArgonSettings,
)
class ServiceSettings(BaseSettings):
    env: Literal["dev", "prod"] = "dev"
    app_name: str = "SMO Loss Simulator API"
    api_prefix: str = "/api"
    cors_origins: list[AnyHttpUrl] = []
    cors_allow_credentials: bool = True
    db_url: str
    model_config = SettingsConfigDict(
        env_file=os.path.join(os.path.dirname(__file__), ".service.env")
    )
    @property
    def debug(self) -> bool:
        return self.env == "dev"
@lru_cache()
def get_service_settings() -> ServiceSettings:
    return ServiceSettings()
class Settings(BaseSettings):
    service: ServiceSettings = Field(default_factory=get_service_settings)
    authorization: AuthorizationSettings = Field(
        default_factory=get_authorization_config
    )
    logger_settings: LoggerSettings = Field(default_factory=get_logger_settings)
    argon_settings: ArgonSettings = Field(default_factory=get_argon_settings)
    def model_post_init(self, context: Any, /) -> None:
        super().model_post_init(context)
        AuthJWT.load_config(get_authorization_config)
@lru_cache
def get_settings() -> Settings:
    return Settings()
</file>

</files>
